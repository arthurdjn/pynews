---
title: "Assignment 1 - IN5550"
author: "Lotte Boerboom, Arthur Dujardin, Sigurd Hylin"
date: "1/31/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Introduction

This report concerns news article classification using Bag of Words. A feed forward neural network was trained to predict one of the 20 sources. A PyNews package implemented in python was used to explore the different possibilities and training hyper parameters, with the help of PyTorch.

We have trained and compared five different architectures on a training and development set. The main focus of this report is to analyse the influence of the number of hidden layers on the model's performance and time efficiency.

From these different structures, we selected the best one and trained it three times and will be used to predict unseen data.


# Data Processing

A data module was used to analyse and extract features from the documents. First, the dataset *The Signal Media One-Million News Articles* which contains articles from september 2015 was used as training data. Then, the raw data was stripped (tokenized) of part of speech tags, and the most common words were saved under a vocabulary vectorizer (`vectorizer.pickle`) of size 3000, which can be used also on test and unknown data.
The document's source (gold labels) are used as target labels to evaluate the predictions.

The functionality for processing the data can be found in the Python package called 'data.py'. This code reads the datafile, stripping the POS tags from the documents, builds the vocabulary and set of labels, and then maps the vocabulary to the documents and encodes the sources to their numeric labels.


# Models


## Feature Tuning

Before changing the structure of the model, we explored differents Bag of Words features implementation by varying the vocabulary size, the preprocessing and building the vocabulary before and after the split. For example, with a vocabulary size of 4000 we did not see improvement in the performance. In addition, using the Part of Speech (POS) tags did not help to optimize the results.
Because of difficulties and time restriction we did not create the vocabulary after splitting into training and development parts. 


## Hyperparameters

A brief training session to evaluate the performance with different hyper parameters was firstly performed. The hyper parameters used are described on the table below.


: Hyperparameters

+----------------------------------+--------------------+
| Parameter                        | Value              |
+==================================+====================+
| Split                            | Train, Dev = .9, .1|
+----------------------------------+--------------------+
| Vocabulary                       | 3000               |
+----------------------------------+--------------------+
| Batch size                       | 32                 |
+----------------------------------+--------------------+
| Learning Rate                    | .09                |
+----------------------------------+--------------------+
| Epochs                           | 250                |
+----------------------------------+--------------------+


## Architectures

Then, five different models were trained on Saga's server with different layout. These models differ in their number of hidden layers, and their architectures are presented in the tables below.


: Model 1 Architecture

+----------------------------------+-------------------+-------------------+
| Layers                           | Neurons           | Activation        |
+==================================+===================+===================+
| Input                            | 3000              | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 1                   | 150               | Linear            |
+----------------------------------+-------------------+-------------------+
| Output                           | 20                | Softmax           |
+----------------------------------+-------------------+-------------------+

: Model 2 Architecture

+----------------------------------+-------------------+-------------------+
| Layers                           | Neurons           | Activation        |
+==================================+===================+===================+
| Input                            | 3000              | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 1                   | 150               | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 2                   | 150               | Linear            |
+----------------------------------+-------------------+-------------------+
| Output                           | 20                | Softmax           |
+----------------------------------+-------------------+-------------------+

: Model 3 Architecture

+----------------------------------+-------------------+-------------------+
| Layers                           | Neurons           | Activation        |
+==================================+===================+===================+
| Input                            | 3000              | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 1                   | 150               | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 2                   | 150               | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 3                   | 150               | Linear            |
+----------------------------------+-------------------+-------------------+
| Output                           | 20                | Softmax           |
+----------------------------------+-------------------+-------------------+

: Model 4 Architecture

+----------------------------------+-------------------+-------------------+
| Layers                           | Neurons           | Activation        |
+==================================+===================+===================+
| Input                            | 3000              | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 1                   | 150               | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 2                   | 150               | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 3                   | 150               | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 4                   | 150               | Linear            |
+----------------------------------+-------------------+-------------------+
| Output                           | 20                | Softmax           |
+----------------------------------+-------------------+-------------------+

: Model 5 Architecture

+----------------------------------+-------------------+-------------------+
| Layers                           | Neurons           | Activation        |
+==================================+===================+===================+
| Input                            | 3000              | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 1                   | 150               | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 2                   | 150               | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 3                   | 150               | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 4                   | 150               | ReLU              |
+----------------------------------+-------------------+-------------------+
| Hidden Layer 5                   | 150               | Linear            |
+----------------------------------+-------------------+-------------------+
| Output                           | 20                | Softmax           |
+----------------------------------+-------------------+-------------------+


# Evaluation

After we trained all models, there was no specific model that stood out. However, we tried to differenciate them regarding four indicators : the accuracy, macro-F1, precision and recall.
$$ Precision = \frac{TP}{TP+ FP} $$
$$ Recall = \frac{TP}{TP+ FN} $$

$$Accuracy = \frac{\#Correct}{N}$$

And Macro-F1 is the weighted average between precision and recall.

## Results

As shown in the table below, the model 4 presents the best accuracy. Nevertheless, this might not be the only criterion to consider, especially because of the different sources frequency. The Macro-F1 score might be less sensitive to imbalanced class frequencies. In that case, the model 3 performs better even if its precision is not optimal. Due to these performance, we choose the model 3 to push further the training.


+----------------+----------+------------+------------+------------+------------------+
| Model          | Accuracy | Macro-F1   | Precision  | Recall     |  Run Time        |
+================+==========+============+============+============+==================+
| Model 1        | 53.33    | 38.32      | 39.04      | 42.11      | 00:22:27         |
+----------------+----------+------------+------------+------------+------------------+
| Model 2        | 52.35    | 32.93      | 36.84      | 31.58      | 00:25:43         |
+----------------+----------+------------+------------+------------+------------------+
| Model 3        | 52.81    | 43.59      | 41.27      | 49.44      | 00:27:04         |
+----------------+----------+------------+------------+------------+------------------+
| Model 4        | 53.55    | 38.75      | 42.22      | 44.11      | 00:28:40         |
+----------------+----------+------------+------------+------------+------------------+
| Model 5        | 53.52    | 42.00      | 46.80      | 43.27      | 00:25:41         |
+----------------+----------+------------+------------+------------+------------------+

[Select the best one, train 3 times, report average of scores, and stddev]:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

mods <- c(1, 2, 3, 4, 5)
accs <- c(53.33, 52.35, 52.81, 53.55, 53.52)
f1s <-  c(38.32, 32.93, 43.59, 38.75, 42.00)
prec <- c(39.04, 36.84, 41.27, 42.22, 46.80)
rec <-  c(42.11, 31.58, 49.44, 44.11, 43.27)
rtim <- c("00:22:27", "00:25:43", "00:27:04", "00:28:40", "00:25:41")

m <- tibble(HiddenLayers=mods, 
            Accuracy=accs,
            MacroF1=f1s,
            Precision=prec,
            Recall=rec,
            RunTime=hms::as.hms(rtim)) %>%
  mutate(RunTime=as.numeric(lubridate::hms(RunTime)) / 60) 

m %>% 
  gather(key="Metric", value="Value", -HiddenLayers) %>%
  ggplot(aes(x = HiddenLayers, y = Value) ) + 
  geom_line(aes(color = Metric)) + 
  geom_point(aes(color = Metric)) + 
  facet_wrap(~Metric, scales = "free_y", ncol=1, 
             strip.position = "top", 
             labeller = as_labeller(c(HiddenLayers="Number of Hidden Layers", 
                                      Accuracy="Accuracy, %",
                                      MacroF1="Macro-F1, %",
                                      Precision="Precision, %",
                                      Recall="Recall, %",
                                      RunTime="Minutes Running Time")))  +
  ylab(NULL) +
  xlab("Number of Hidden Layers") +
  theme(legend.position = "none")

```


## Model Training

The mean and standard deviations of the metrics when running the chosen model three times is displayed in the table below.

```{r echo=FALSE, message=FALSE, warning=FALSE}
accs <- c(53.07, 53.63, 53.24)
prec <- c(29.76, 38.02, 33.78)
rec <-  c(30.95, 40.42, 40.44)
f1s <-  c(29.52, 37.37, 35.33)

rtim <- c("00:30:57", "00:49:51", "00:47:24")

m <- tibble("Accuracy"=accs,
"Macro-F1"=f1s,
"Precision"=prec,
"Recall"=rec,
RunTime=hms::as.hms(rtim)) %>%
mutate("Run Time"=as.numeric(lubridate::hms(RunTime)) / 60)

m %>%
  select(-RunTime) %>%
  gather(key="Metric", value = "Val") %>%
  group_by(Metric) %>%
  summarize("Average" = mean(Val), "Std.dev" = sd(Val)) %>%
  knitr::kable(longtable = TRUE, booktabs = TRUE,
  caption = 'Averages and Standard Deviations of Final Runs')


```


# Conclusion

As shown in the results, the performance are really different from one training to the other. Most likely this is due to us no setting a random seed, leading to different weights, bias and also affecting the optimizer.











